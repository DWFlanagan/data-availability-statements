{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0.4 Tokenize\n",
    "\n",
    "This notebook will use Spacy to tokenize the answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import engarde.decorators as ed\n",
    "import spacy\n",
    "from spacy.lang.en import English\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.attrs import ORTH, LEMMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJ_ROOT = os.path.join(os.pardir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the filtered data\n",
    "\n",
    "\n",
    "@ed.is_shape((None, 6))\n",
    "def load_data():\n",
    "    PROJ_ROOT = os.path.join(os.pardir)\n",
    "    read_path = os.path.join(PROJ_ROOT + \"/data/interim/\" + \"data_statements.feather\")\n",
    "\n",
    "    df = pd.read_feather(read_path)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_questions = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 197899 entries, 0 to 197898\n",
      "Data columns (total 6 columns):\n",
      "site_name           197899 non-null object\n",
      "documentid          197899 non-null object\n",
      "customquestionid    197899 non-null int64\n",
      "questiontext        197899 non-null object\n",
      "answertext          127714 non-null object\n",
      "submissiondate      191066 non-null datetime64[ns]\n",
      "dtypes: datetime64[ns](1), int64(1), object(4)\n",
      "memory usage: 9.1+ MB\n"
     ]
    }
   ],
   "source": [
    "filtered_questions.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 124089 entries, 0 to 197896\n",
      "Data columns (total 6 columns):\n",
      "site_name           124089 non-null object\n",
      "documentid          124089 non-null object\n",
      "customquestionid    124089 non-null int64\n",
      "questiontext        124089 non-null object\n",
      "answertext          124089 non-null object\n",
      "submissiondate      124089 non-null datetime64[ns]\n",
      "dtypes: datetime64[ns](1), int64(1), object(4)\n",
      "memory usage: 6.6+ MB\n"
     ]
    }
   ],
   "source": [
    "filtered_questions = filtered_questions.dropna(subset=[\"answertext\", \"submissiondate\"])\n",
    "\n",
    "filtered_questions.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add custom stop words to Spacy's list\n",
    "\n",
    "customize_stop_words = [\"wiley\", \"br\", \"href\", \"url\", \"et\", \"al\"]\n",
    "for w in customize_stop_words:\n",
    "    nlp.vocab[w].is_stop = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a special case to not change \"data\" to \"datum\"\n",
    "\n",
    "case = [{ORTH: \"data\", LEMMA: \"data\"}]\n",
    "nlp.tokenizer.add_special_case(\"data\", case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_tokenizer(sentence):\n",
    "\n",
    "    doc = nlp(sentence)\n",
    "\n",
    "    mytokens = [\n",
    "        token.lemma_\n",
    "        for token in doc\n",
    "        if token.pos_ in [\"NOUN\", \"ADJ\", \"PROPN\"]\n",
    "        and not token.is_stop\n",
    "        and not token.like_url\n",
    "    ]\n",
    "\n",
    "    prepared_text = \" \".join(mytokens)\n",
    "\n",
    "    return prepared_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 19min 55s, sys: 18.7 s, total: 20min 14s\n",
      "Wall time: 22min 31s\n"
     ]
    }
   ],
   "source": [
    "%time filtered_questions[\"proc_answers\"] = filtered_questions[\"answertext\"].apply(spacy_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_statements' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-f941cd2a02f3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msave_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPROJ_ROOT\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/data/processed/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'tokenized.feather'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdata_statements\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_feather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'data_statements' is not defined"
     ]
    }
   ],
   "source": [
    "save_path = os.path.join(PROJ_ROOT + '/data/processed/' + 'tokenized.feather')\n",
    "\n",
    "data_statements.reset_index(drop=True).to_feather(save_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:data-availability]",
   "language": "python",
   "name": "conda-env-data-availability-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
